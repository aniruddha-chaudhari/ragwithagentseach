{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "def prepare_document(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prepares the document and gets initial response\n",
    "    Returns: Dictionary with raw response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Google AI client\n",
    "        client = genai.Client(api_key=\"AIzaSyD4lR1WQ1yaZumSFtMVTG_0Y8d0oRy1XhA\")\n",
    "        \n",
    "        # Upload the file\n",
    "        uploaded_file = client.files.upload(file=file_path)\n",
    "        \n",
    "        # First prompt for general analysis\n",
    "        initial_prompt = \"\"\"\n",
    "        describe the image in detail. Include the following information in your response:\n",
    "      Primary Subject and Setting: Identify the main subject(s) of the image and describe the setting or environment in which they are located.\n",
    "\n",
    "Attributes and Characteristics: Detail the physical attributes, expressions, attire, and other distinguishing features of the primary subject(s).\n",
    "\n",
    "Activities and Interactions: Explain any actions, events, or interactions taking place among subjects or between subjects and their environment.\n",
    "\n",
    "Artistic Elements: Describe the artistic style, lighting, color palette, perspective, and any other visual elements that contribute to the overall aesthetic of the image.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get initial response\n",
    "        initial_response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[uploaded_file, initial_prompt]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True, \n",
    "            \"uploaded_file\": uploaded_file,\n",
    "            \"initial_response\": initial_response.text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"Error preparing document: {str(e)}\"}\n",
    "    \n",
    "response = prepare_document(\"F:/Aniruddha/code/webdev/PROJECTS/teacherassistant/backend/images.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': True, 'uploaded_file': File(name='files/7kn2mgnxhu10', display_name=None, mime_type='image/png', size_bytes=6988, create_time=datetime.datetime(2025, 3, 3, 19, 13, 22, 376205, tzinfo=TzInfo(UTC)), expiration_time=datetime.datetime(2025, 3, 5, 19, 13, 22, 361286, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 3, 3, 19, 13, 22, 376205, tzinfo=TzInfo(UTC)), sha256_hash='MWU2OWYyMjM3ZWU4OWEyZWY2ZGMyZjVhYWVhZGFiMWFkOTg2NjY3MjQxNGZjMzBiOTViYWMzOGZiMjUzNzdkOA==', uri='https://generativelanguage.googleapis.com/v1beta/files/7kn2mgnxhu10', download_uri=None, state=<FileState.ACTIVE: 'ACTIVE'>, source=<FileSource.UPLOADED: 'UPLOADED'>, video_metadata=None, error=None), 'initial_response': 'Here\\'s a structured analysis of the cartoon lion image:\\n\\n**1. Overall Description of the Image:**\\n\\nThe image depicts a cartoon illustration of a lion standing in a slightly angled, three-quarter view. It\\'s a simple, cheerful representation suitable for children. The style is clean and uses bold outlines and flat colors.\\n\\n**2. Key Elements Present in the Image:**\\n\\n*   **Lion:** The central figure, characterized by:\\n    *   A large, brown mane.\\n    *   A light tan body.\\n    *   Large, round eyes with small pupils.\\n    *   A small, smiling mouth.\\n    *   A small tail with a slight curl at the end.\\n    *   Well-defined paws.\\n    *   Whiskers\\n\\n*   **Color Palette:** The image uses a limited color palette consisting of brown (for the mane), tan (for the body), off-white (for the muzzle and paws), and black (for outlines and pupils).\\n*   **Outlines:** Thick, black outlines define the shapes, making it visually clear.\\n*   **White Background:** The image is set against a plain white background, isolating the lion.\\n\\n**3. Quality Assessment:**\\n\\n*   **Clarity:** The image is very clear and sharp. The outlines are well-defined, and the colors are distinct.\\n*   **Composition:** The composition is simple and effective. The lion is centered and takes up most of the frame, ensuring it\\'s the main focus. The angle gives a good view of the lion\\'s features.\\n*   **Lighting:** The image doesn\\'t depict any specific lighting. The colors are flat and even, suggesting a cartoon style rather than realistic lighting.\\n\\n**4. Potential Educational Use Cases:**\\n\\n*   **Early Childhood Education:**\\n    *   Identifying animals (learning about lions).\\n    *   Color recognition (brown, tan, white, black).\\n    *   Basic shapes (circles, ovals).\\n    *   Storytelling and character development.\\n*   **Art Classes:**\\n    *   Learning about cartooning and illustration.\\n    *   Understanding line art and color blocking.\\n    *   Simple character design.\\n*   **Language Learning:**\\n    *   Introducing animal names in a new language.\\n    *   Using the image to create simple sentences (e.g., \"This is a lion.\").\\n*   **Nature/Zoology:**\\n    *   Introduction to wild animals.\\n    *   Briefly mentioning the natural habitat and diet of lions.\\n\\n**5. Concerns or Issues:**\\n\\n*   **Oversimplification:** The image is highly simplified, which could be a limitation for older audiences or for more in-depth educational purposes. It doesn\\'t represent the true appearance or behavior of a lion in detail.\\n*   **Stereotyping:** While this is a minor concern, overly anthropomorphized animal images can sometimes perpetuate stereotypes. In this case, the smiling lion portrays a friendly nature, which might not reflect a realistic understanding of lions as potentially dangerous wild animals. However, given the target audience, the cartoon nature is likely acceptable.\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:\n",
      "Marks: 65\n",
      "Remarks: ['The paper has a reasonably clear structure, with defined sections for theory, code, and output.', 'The provided C code appears to implement a shift-reduce parser, which is the stated goal.', 'Including the example output is good practice as it helps demonstrate how the parser works with a specific input.', 'The \"Theory\" section provides a decent overview of shift-reduce parsing concepts.']\n",
      "Suggestions: ['While the theory section introduces the concepts, it could benefit from more in-depth explanations and examples.', 'The C code lacks comments, making it harder to understand the logic behind each step, especially for readers unfamiliar with the algorithm. Add inline comments.', \"The code doesn't appear to have robust error handling. It could be enhanced to detect and report syntax errors.\", \"The program takes grammar rules as input. It's critical to specify the format and restrictions of the accepted grammar. Is it context-free? What are the limitations on production rules? This should be explicitly stated.\", 'There is very little validation on the inputs received from the user which could cause the program to crash.', 'Include more detailed explanations of shift-reduce parsing.', 'Give concrete examples of how shift and reduce operations work with a specific grammar.', 'Visually demonstrate how the parse tree is built.', 'Add comments to explain each part of the algorithm.', 'Add robust error handling to catch invalid input and other issues.', 'Implement checks to prevent buffer overflows in the `stack` array.', 'Validate user-provided grammar rules to ensure they are in the correct format and within acceptable limits.', 'Use more descriptive variable names to improve code readability.', 'Provide more information on how the language grammar is defined.', 'Clean up the coding style, including indentations.', 'Test it more thoroughly with a variety of grammars and input strings, including edge cases and invalid inputs.', 'Improve variable names, add comments, and restructure for readability.']\n",
      "Errors: [\"The `stack[++top] = input[ip];` operation in the `while` loop within the `main` function is vulnerable to buffer overflow if the input string is longer than `MAX - 1`. There's no check to prevent `top` from exceeding the array bounds of `stack`.\", \"The code uses `strncmp`, which is a function typically declared in `<string.h>`. Although the header file is declared, it seems odd it wasn't recognized during OCR.\", 'The parser\\'s \"Error\" operation is only mentioned in the theory section. There is no obvious way that error is handled in the code.', 'The grammar rules are entered through the command line. There is no guarantee that the language defined by the grammar will be parsed correctly.', 'The code currently only handles single-character non-terminals.', 'The `printf` statements within the `printState` function are missing a `\\\\n` character at the end of each format string.', 'In the Output section, the \"!!!\" is likely from the poorly-formatted `printState` function.']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class PaperCheckResult(BaseModel):\n",
    "    marks: int\n",
    "    remarks: List[str]\n",
    "    suggestions: List[str]\n",
    "    errors: List[str]\n",
    "\n",
    "def prepare_document(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prepares the document and gets initial response\n",
    "    Returns: Dictionary with raw response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Google AI client\n",
    "        client = genai.Client(api_key=\"AIzaSyD4lR1WQ1yaZumSFtMVTG_0Y8d0oRy1XhA\")\n",
    "        \n",
    "        # Upload the file\n",
    "        uploaded_file = client.files.upload(file=file_path)\n",
    "        \n",
    "        # First prompt for general analysis\n",
    "        initial_prompt = \"\"\"\n",
    "        Analyze this academic paper and provide feedback. Include:\n",
    "        1. Overall quality score (0-100)\n",
    "        2. Positive aspects of the paper\n",
    "        3. Areas that need improvement\n",
    "        4. Any errors or problems found\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get initial response\n",
    "        initial_response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[uploaded_file, initial_prompt]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True, \n",
    "            \"uploaded_file\": uploaded_file,\n",
    "            \"initial_response\": initial_response.text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"Error preparing document: {str(e)}\"}\n",
    "\n",
    "def analyze_document(initial_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Takes initial response and converts it to structured format\n",
    "    Returns: Dictionary with structured results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not initial_result[\"success\"]:\n",
    "            return initial_result\n",
    "            \n",
    "        client = genai.Client(api_key=\"AIzaSyD4lR1WQ1yaZumSFtMVTG_0Y8d0oRy1XhA\")\n",
    "        \n",
    "        # Second prompt to structure the response\n",
    "        structure_prompt = f\"\"\"\n",
    "        Convert the following feedback into a structured JSON format:\n",
    "\n",
    "        {initial_result['initial_response']}\n",
    "\n",
    "        The JSON should have this structure:\n",
    "        {{\n",
    "            \"marks\": integer (0-100),\n",
    "            \"remarks\": [list of positive comments],\n",
    "            \"suggestions\": [list of improvement areas],\n",
    "            \"errors\": [list of problems found]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get structured response\n",
    "        structured_response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=structure_prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        data = structured_response.text\n",
    "        if isinstance(data, str):\n",
    "            data = eval(data)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            data = [data]\n",
    "            \n",
    "        results = [PaperCheckResult(**item) for item in data]\n",
    "        return {\"success\": True, \"results\": [r.model_dump() for r in results]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def process_document(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function that coordinates the document processing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First get raw analysis\n",
    "        initial_result = prepare_document(file_path)\n",
    "        if not initial_result[\"success\"]:\n",
    "            return initial_result\n",
    "            \n",
    "        # Then convert to structured format\n",
    "        return analyze_document(initial_result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'spcc3.pdf'\n",
    "    result = process_document(file_path)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(\"Analysis Results:\")\n",
    "        for paper_result in result[\"results\"]:\n",
    "            print(f\"Marks: {paper_result['marks']}\")\n",
    "            print(f\"Remarks: {paper_result['remarks']}\")\n",
    "            print(f\"Suggestions: {paper_result['suggestions']}\")\n",
    "            print(f\"Errors: {paper_result['errors']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"Error:\", result[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pocso case: 20 years’ RI for father after fearless deposition by Thane girl | Mumbai News - The Times of India\n",
      "\n",
      "Number of content chunks: 14\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Pocso case: 20 years’ RI for father after fearless deposition by Thane girl | Mumbai News - The Times of IndiaEditionININUSSign InTOICitymumbaimumbai ...\n",
      "\n",
      "Chunk 2:\n",
      "20 years’ RI for father after fearless deposition by Thane girlTrendingUttarakhand AvalancheHimani Narwal MurderAyodhya Ram Mandir FootwearAmroha Cat ...\n",
      "\n",
      "Chunk 3:\n",
      "rigorous imprisonment (RI).Judge DS Deshmukh imposed a fine of Rs 20,000 and directed that it be disbursed to the victim as compensation and furthermo...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_web_document(url):\n",
    "    \"\"\"\n",
    "    Load a document from a specified URL using WebBaseLoader.\n",
    "\n",
    "    Args:\n",
    "    - url (str): The URL of the webpage to load.\n",
    "\n",
    "    Returns:\n",
    "    - docs: The loaded document(s).\n",
    "    \"\"\"\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def extract_title_and_split_content(docs):\n",
    "    \"\"\"\n",
    "    Extract title and split content into chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "    - docs: The documents loaded from WebBaseLoader.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (title, content_chunks)\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return None, []\n",
    "    \n",
    "    # Get title from the first document's metadata\n",
    "    title = docs[0].metadata.get('title')\n",
    "    \n",
    "    # Add source metadata\n",
    "    for doc in docs:\n",
    "        doc.metadata.update({\n",
    "            \"source_type\": \"url\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    # Apply text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    content_chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return title, content_chunks\n",
    "\n",
    "# Example usage\n",
    "url = \"https://timesofindia.indiatimes.com/city/mumbai/pocso-case-20-years-ri-for-father-after-fearless-deposition-by-thane-girl/articleshow/118688947.cms\"\n",
    "docs = load_web_document(url)\n",
    "title, content_chunks = extract_title_and_split_content(docs)\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"\\nNumber of content chunks: {len(content_chunks)}\")\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(content_chunks[:3]):  # Print first 3 chunks as sample\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk.page_content[:150] + \"...\")  # Print first 150 chars of each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Raw document count: 1\n",
      "📝 Sample content: ...\n",
      "Extracted 0 chunks from the webpage\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import bs4\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def process_web(url: str, chunk_size: int = 1000, chunk_overlap: int = 200, \n",
    "                specific_classes: Optional[List[str]] = None) -> list:\n",
    "    \"\"\"\n",
    "    Process web URL and extract content with metadata.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to process\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        specific_classes: Optional list of CSS classes to target\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine if we should use targeted parsing or general parsing\n",
    "        bs_kwargs = {}\n",
    "        if specific_classes:\n",
    "            bs_kwargs = {\n",
    "                \"parse_only\": bs4.SoupStrainer(\n",
    "                    class_=tuple(specific_classes)\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        # Load and parse webpage content\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url,),\n",
    "            bs_kwargs=bs_kwargs\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"📄 Raw document count: {len(documents)}\")\n",
    "        if documents:\n",
    "            print(f\"📝 Sample content: {documents[0].page_content[:200]}...\")\n",
    "        else:\n",
    "            print(\"⚠️ No content extracted. Trying again with general parsing...\")\n",
    "            # Fallback to general parsing\n",
    "            loader = WebBaseLoader(web_paths=(url,))\n",
    "            documents = loader.load()\n",
    "            if documents:\n",
    "                print(f\"📄 Raw document count after fallback: {len(documents)}\")\n",
    "                print(f\"📝 Sample content: {documents[0].page_content[:200]}...\")\n",
    "        \n",
    "        # Add source metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"source_type\": \"web_page\",\n",
    "                \"url\": url,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"title\": extract_title_from_url(url)\n",
    "            })\n",
    "            \n",
    "        # Split documents into manageable chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        return chunks\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"🌐 Web processing error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_title_from_url(url: str) -> str:\n",
    "    \"\"\"Extract a readable title from a URL\"\"\"\n",
    "    # Remove protocol and domain\n",
    "    path = re.sub(r'^https?://[^/]+/', '', url)\n",
    "    # Remove query parameters and fragments\n",
    "    path = re.sub(r'[?#].*$', '', path)\n",
    "    # Replace underscores and hyphens with spaces\n",
    "    title = re.sub(r'[-_]', ' ', path)\n",
    "    # Split by slashes and take the last meaningful part\n",
    "    parts = [p for p in title.split('/') if p]\n",
    "    return parts[-1].title() if parts else \"Web Page\"\n",
    "\n",
    "# Test the function with a Wikipedia page\n",
    "common_content_classes = [\"content\", \"main\", \"article\", \"post\", \"entry\", \"page-content\", \"body-content\"]\n",
    "web_chunks = process_web(\"https://en.wikipedia.org/wiki/Elden_Ring\", \n",
    "                         specific_classes=common_content_classes)\n",
    "print(f\"Extracted {len(web_chunks)} chunks from the webpage\")\n",
    "if web_chunks:\n",
    "    print(f\"First chunk sample: {web_chunks[0].page_content[:100]}...\")\n",
    "    print(f\"Metadata: {web_chunks[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "Today, March 3, 2025, the Supreme Court reprimanded comedian Samay Raina for commenting on the ongoing case involving his show 'India's Got Latent' while in Canada. Justice Surya Kant stated, \"These youngsters may think that we are outdated, but we know how to deal with them. Don't take the court lightly.\" The court felt that Raina was being \"oversmart\" and didn't understand the Court's jurisdiction.\n",
      "\n",
      "\n",
      "Source Links:\n",
      "1. https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrx025Ewkb5yZl56U3FxsImwts2FXfzn9WN1XmQvhjr8zj6sY3t9TRT5phE57dgCkFWhpe-IOIBI4ctWdV3cHGFxTC6mMVC-v6LsPOVT_BxWGP7rjtLF4MM_UpkQgoNzFBdptBc-1j5OBjeFy502Vep4QTuRoCoNtldBVynDlnqwvZXxtH4vdn_SVCYH4pYl05XsK9GLTzA-QW-NRVLcgJigxYhC8T3-8ALEp6WlJ-8bupg5ukRHqI1KpgyUwiBcujSkc-sXG5MLUYHb7IkJyal7\n",
      "2. https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrwpmE-N4-pSYbmG7mvz5Zeayy32NLnSF-JUmm80p4usjPu2DaaAz0TaVwN9zKELmlFuTcMiSxgGv5znPkwjL8UFmUYE5oMr4N7A3rNbHzqjvSYIMUk6ZsupI-yGWCWciQMJ5ZpYUr9yOA2nGCzOi7YcZ8SCJK9nRSVFPjBcLlhrLm0xb3Heb2UIEjJ-WPeHeVY_-zWhDnJOm844xNhqEO0yCARal0ZkGUxlwTmMoemoey0z02pVD0FXe3b5WO9AXAc-BVuQPyQ0uw==\n",
      "3. https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzupSij8W8VCx-3fH7OntRMKvtG1O8z1AFXJKtOOCV0FtWimMikAajR5_yeI2zKioUIXrZY3NNrlPpIFIMU3cN-nK_W1EPL1NrMyx2G-J-4c3I_4WUshxkuOYdnlLMiVjjfm2UFChYnBLnIBL1wRLQIE_-L9tcIcylZo6LcGEDjYPjnGUhPK380PRnUHcj9KygHDoF81K4tXvy_BrmvSTYqSWlR3Bjvz78M6ExjEhomGLodl-pFwTTk06J-diG-IDUuUTRjglb0jC0n\n"
     ]
    }
   ],
   "source": [
    "def google_search(query: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform a Google search using Gemini's built-in search capability.\n",
    "    Returns a tuple containing (text_response, search_links)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=query,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.2,\n",
    "                tools=[types.Tool(\n",
    "                    google_search=types.GoogleSearchRetrieval()\n",
    "                )]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract links from citations and grounding metadata\n",
    "        links = []\n",
    "        if hasattr(response, 'candidates') and response.candidates:\n",
    "            for candidate in response.candidates:\n",
    "                # Extract links from citation metadata\n",
    "                if hasattr(candidate, 'citation_metadata') and candidate.citation_metadata:\n",
    "                    for citation in candidate.citation_metadata.citations:\n",
    "                        if hasattr(citation, 'url') and citation.url:\n",
    "                            links.append(citation.url)\n",
    "                # Extract links from grounding metadata\n",
    "                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:\n",
    "                    for chunk in candidate.grounding_metadata.grounding_chunks:\n",
    "                        if hasattr(chunk, 'web') and chunk.web:\n",
    "                            links.append(chunk.web.uri)\n",
    "        \n",
    "        return response.text, links\n",
    "    except Exception as e:\n",
    "        print(f\"🔍 Google search error: {str(e)}\")\n",
    "        return \"\", []\n",
    "\n",
    "# Get both text and links\n",
    "search_text, search_links = google_search(\"What Supreme court said to samay raina about his case today\")\n",
    "\n",
    "print(\"Search Results:\")\n",
    "print(search_text)\n",
    "print(\"\\nSource Links:\")\n",
    "for i, link in enumerate(search_links, 1):\n",
    "    print(f\"{i}. {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\", \"AIzaSyD4lR1WQ1yaZumSFtMVTG_0Y8d0oRy1XhA\")\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "class GoogleSearchIntentResult(BaseModel):\n",
    "    requires_search: bool\n",
    "\n",
    "\n",
    "\n",
    "def detect_google_search_intent(query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the user's query requires internet access to answer properly.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the query requires internet access, False otherwise\n",
    "    \"\"\"\n",
    "   \n",
    "    prompt = f\"\"\"You are an expert at determining when a query requires up-to-date information from the internet.\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Analyze the following user query\n",
    "    2. Determine if an LLM without internet access can provide a satisfactory answer\n",
    "    3. If the query likely needs internet access (e.g., current events, specific data, recent information), return: {{\"requires_search\": true}}\n",
    "    4. If the query can be answered without internet access (e.g., general knowledge, coding help), return: {{\"requires_search\": false}}\n",
    "    \n",
    "    Examples requiring internet:\n",
    "    - Current news or events\n",
    "    - Recent statistics or data\n",
    "    - Real-time information (weather, stocks)\n",
    "    - Specific factual lookups that aren't common knowledge\n",
    "    - Information that changes frequently\n",
    "    \n",
    "    Return ONLY the JSON object without any additional text.\n",
    "    \n",
    "    User query: {query}\n",
    "    \"\"\"\n",
    "     \n",
    "    try:\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json',\n",
    "                'response_schema': GoogleSearchIntentResult,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        result = response.parsed\n",
    "        return result.requires_search\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting search intent: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires search: True\n"
     ]
    }
   ],
   "source": [
    "result = detect_google_search_intent(\"What happened today in india?\")\n",
    "print(f\"Requires search: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
